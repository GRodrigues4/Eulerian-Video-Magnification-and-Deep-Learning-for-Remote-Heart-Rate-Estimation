{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GRodrigues4/EVM-and-Deep-Learning-for-Remote-HR-Estimation/blob/main/EVM_HR_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6S6AsfejDxc"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qw7mQsYqx6MQ"
      },
      "outputs": [],
      "source": [
        "# Install all the needed packages\n",
        "!pip install mediapipe\n",
        "\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bjnE6lbmjIA9"
      },
      "outputs": [],
      "source": [
        "# Import all the necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import scipy.fftpack as fftpack\n",
        "import scipy.signal as signal\n",
        "import scipy.interpolate as interpolate\n",
        "from scipy import stats\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbpPChsnyVbC"
      },
      "outputs": [],
      "source": [
        "# Give access to the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import the Mediapipe model\n",
        "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU2gJNY-i2Dm"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnM_mQjpxmQ2"
      },
      "source": [
        "### Signal Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrI1-2M_8AC-"
      },
      "outputs": [],
      "source": [
        "class SignalProcessing:\n",
        "  def __init__(self, file, time_stamps = [], split_time = 20, sample_rate = 1000):\n",
        "\n",
        "    # Constructor to initialize the SignalProcessing class with parameters:\n",
        "    # file: The file name containing the data.\n",
        "    # time_stamps: A list of timestamps to be skipped.\n",
        "    # split_time: The duration (in seconds) for which HR is calculated (each interval).\n",
        "    # sample_rate: The rate at which data is sampled, in Hz (samples per second).\n",
        "\n",
        "    self.file_name = file\n",
        "    self.time_stamps = time_stamps\n",
        "    self.split_time = split_time\n",
        "    self.sample_rate = sample_rate\n",
        "\n",
        "\n",
        "  def cut(self):\n",
        "\n",
        "    # Method to preprocess the data by selecting a portion of the ECG signal\n",
        "    # based on the condition that there is no '1' found in a specific range of 'In' values.\n",
        "    # Returns the preprocessed ECG and Input (button for sync) signals.\n",
        "\n",
        "    data = np.loadtxt(self.file_name) # Load the data from the file.\n",
        "    ECG = []  # Initialize an empty list to store ECG signal values.\n",
        "    In = []   # Initialize an empty list to store 'In' signal values.\n",
        "\n",
        "    # Iterate through each row in the data.\n",
        "    for row in data:\n",
        "      ECG.append(row[5])  # Append the 6th column (ECG) to the ECG list.\n",
        "      In.append(row[1])   # Append the 2nd column (In) to the In list.\n",
        "\n",
        "    # Iterate through ECG data in steps of 100.\n",
        "    for index in range(0,len(ECG),50):\n",
        "      # Check if there is no '1' in the 'In' signal within the specified range.\n",
        "      if not any(element == 1 for element in In[index:(index+60*self.sample_rate)]):\n",
        "        # Select the ECG and In data for the next 10 minutes (600 seconds).\n",
        "        ECG = ECG[index:(index+60*10*self.sample_rate)]\n",
        "        In = In[index:(index+60*10*self.sample_rate)]\n",
        "        break # Exit the loop once the desired segment is found.\n",
        "\n",
        "    return ECG, In\n",
        "\n",
        "\n",
        "  def meanHR(self, ECG):\n",
        "\n",
        "    # Method to calculate the mean heart rate (HR) from the ECG signal.\n",
        "    # Returns the mean HR value.\n",
        "\n",
        "    ECG = np.array(ECG)\n",
        "    ECG = ECG - np.mean(ECG)\n",
        "    ECG = (ECG/np.max(ECG))\n",
        "\n",
        "    ECG[ECG < 0.4] = 0  # Apply a threshold to eliminate low-amplitude noise.\n",
        "    distance = 0.4*self.sample_rate   # Minimum distance between peaks.\n",
        "\n",
        "    # Detect peaks in the ECG signal.\n",
        "    peaks, _ = signal.find_peaks(ECG, distance=distance)\n",
        "\n",
        "    # Calculate the RR intervals (time difference between successive peaks).\n",
        "    rr_intervals = np.diff(peaks)/self.sample_rate\n",
        "\n",
        "    # Calculate heart rate in beats per minute (BPM).\n",
        "    heart_rate = 60 / rr_intervals\n",
        "\n",
        "    # Calculate and return the mean HR rounded to the nearest integer.\n",
        "    mean_hr = round(np.mean(heart_rate))\n",
        "\n",
        "    return int(mean_hr)\n",
        "\n",
        "\n",
        "  def save(self, array, name):\n",
        "\n",
        "    # Method to save the calculated HR values to a text file.\n",
        "    # Takes the HR values and the desired file name as parameters.\n",
        "    # The HR values are saved in a format suitable for further analysis.\n",
        "\n",
        "    # Save the array with HR values to a text file, along with a descriptive header.\n",
        "    np.savetxt(name, array, fmt='%i', delimiter='\\t', header='Mean Heart Rate (HR) of each interval video, calculated using'\n",
        "      + ' the time difference between R-R peaks. Each value corresponds to an interval of ' + str(self.split_time)\n",
        "               + ' seconds of the respective subject (i.e. the interval with index 0 corresponds to the mean HR between 0 and '\n",
        "               +  str(self.split_time) + ' seconds of recording and so on).\\nThis file is in a format appropriate to read with '\n",
        "               + 'Python NumPy \"loadtxt\" function.\\nThe left values correspond to the index of the interval and the right '\n",
        "               + 'column has the values of the measured mean HR. In case of 0 value it means the interval was removed due '\n",
        "               + 'to severe artifacts of image/signal.\\n\\tHR')\n",
        "\n",
        "\n",
        "  def run(self, output_file = \"output_file.txt\"):\n",
        "\n",
        "    # Method to run the complete processing pipeline: cutting the data, calculating HR, and saving the results.\n",
        "\n",
        "    ECG, In = self.cut()  # Cut and preprocess the data.\n",
        "    HR_values = []        # Initialize a list to store the HR values for each interval.\n",
        "    time = self.split_time*self.sample_rate   # Calculate the number of samples in each interval.\n",
        "\n",
        "    # Generate a list of indexes corresponding to the given timestamps.\n",
        "    indexes = [((index*self.sample_rate)//time)*time for index in self.time_stamps]\n",
        "\n",
        "    # Iterate through the ECG signal in steps of 'time' (one interval at a time).\n",
        "    for i in range(0, len(ECG), time):\n",
        "      if i in indexes:    # Check if the current interval should be skipped.\n",
        "        HR_values.append([i//time,0])   # Append a zero HR value for skipped intervals.\n",
        "      else:\n",
        "        # Calculate and append the mean HR for the current interval.\n",
        "        HR_values.append([i//time,self.meanHR(ECG[i:i+time])])\n",
        "        np.save(output_file + '_ECG_' + str(i//time), ECG[i:i+time])  # Save the ECG signal\n",
        "\n",
        "    self.save(HR_values, output_file + '_HR.txt') # Save the HR values to the output file."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video Processing"
      ],
      "metadata": {
        "id": "-IKJsgwtDz0m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7P7LP0bnPXE"
      },
      "outputs": [],
      "source": [
        "class VideoSegmentation:\n",
        "  def __init__(self, file_path, time_stamps = [], split_time = 20, fps = 30, image_width = 1280,\n",
        "               image_height = 720, bounding_box_size = 64):\n",
        "\n",
        "    # Constructor to initialize the VideoSegmentation class with parameters:\n",
        "    # file_path: Path to the video file.\n",
        "    # time_stamps: List of timestamps to skip.\n",
        "    # split_time: Duration (in seconds) for segmenting the video into intervals.\n",
        "    # fps: Frames per second of the video.\n",
        "    # image_width: Width of the video frame in pixels.\n",
        "    # image_height: Height of the video frame in pixels.\n",
        "    # bounding_box_size: Size of the bounding box around facial landmarks.\n",
        "\n",
        "    self.file_path = file_path\n",
        "    self.time_stamps = time_stamps\n",
        "    self.split_time = split_time\n",
        "    self.fps = fps\n",
        "    self.image_width = image_width\n",
        "    self.image_height = image_height\n",
        "    self.box_size = bounding_box_size\n",
        "\n",
        "\n",
        "  def relative_to_absolute(self, relative_coords):\n",
        "\n",
        "    # Convert relative coordinates (normalized between 0 and 1) to absolute pixel values.\n",
        "    # Returns a list of absolute coordinates.\n",
        "\n",
        "    absolute_coords = [[int(coord[0] * self.image_width), int(coord[1] * self.image_height)] for coord in relative_coords]\n",
        "\n",
        "    return absolute_coords\n",
        "\n",
        "\n",
        "  def segment_frame(self, frame, time_ms):\n",
        "\n",
        "    # Method to detect facial landmarks in the given frame and return the coordinates of specific landmarks.\n",
        "    # Returns a list of relative coordinates for the specified landmarks.\n",
        "\n",
        "    # Create a face landmarker instance with the video mode in order to extract the facial features\n",
        "    base_options = python.BaseOptions(model_asset_path = 'face_landmarker_v2_with_blendshapes.task')\n",
        "    options = vision.FaceLandmarkerOptions(base_options = base_options, running_mode = mp.tasks.vision.RunningMode.VIDEO)\n",
        "    detector = vision.FaceLandmarker.create_from_options(options)\n",
        "\n",
        "    # Convert the frame to RGB as the model expects RGB input.\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
        "\n",
        "    # Detect facial landmarks for the frame at the given time in milliseconds.\n",
        "    result = detector.detect_for_video(mp_image, round(time_ms))\n",
        "\n",
        "    # Extract specific landmark coordinates: forehead, cheek 1, and cheek 2.\n",
        "    coord = [(result.face_landmarks[0][151].x, result.face_landmarks[0][151].y),(result.face_landmarks[0][50].x, result.face_landmarks[0][50].y)\n",
        "          ,(result.face_landmarks[0][280].x, result.face_landmarks[0][280].y)]\n",
        "    coord = self.relative_to_absolute(coord)\n",
        "\n",
        "    return coord\n",
        "\n",
        "\n",
        "  def create_bounding_box(self, coord):\n",
        "\n",
        "    # Method to create a bounding box around a given coordinate.\n",
        "    # Returns the top-left and bottom-right coordinates of the bounding box.\n",
        "\n",
        "    x_coord = coord[0]\n",
        "    y_coord = coord[1]\n",
        "\n",
        "    # Calculate the bounding box limits ensuring they stay within frame boundaries.\n",
        "    x_min = max(0, x_coord - self.box_size // 2)\n",
        "    y_min = max(0, y_coord - self.box_size // 2)\n",
        "    x_max = min(self.image_width, x_coord + self.box_size // 2)\n",
        "    y_max = min(self.image_height, y_coord + self.box_size // 2)\n",
        "\n",
        "    return (int(x_min), int(y_min)), (int(x_max), int(y_max))\n",
        "\n",
        "\n",
        "  def cut_video(self, frame, coord):\n",
        "\n",
        "    # Method to extract a specific segment of the video frame using a bounding box.\n",
        "    # Returns the segmented frame.\n",
        "\n",
        "    top_left, bottom_right = self.create_bounding_box(coord)\n",
        "    segmented_frame = frame[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
        "\n",
        "    return segmented_frame\n",
        "\n",
        "\n",
        "  def save_video(self, video, output_path):\n",
        "\n",
        "    # Method to save the segmented video frames to a file.\n",
        "    # Takes the segmented frames and the desired output file path as parameters.\n",
        "\n",
        "    # Define the codec and create a VideoWriter object.\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    [image_height, image_width] = video[0].shape[0:2]\n",
        "    writer = cv2.VideoWriter(output_path, fourcc, self.fps, (image_width, image_height), 1)\n",
        "\n",
        "    # Write each frame to the output video.\n",
        "    for i in range(video.shape[0]):\n",
        "        writer.write(cv2.convertScaleAbs(video[i]))\n",
        "    writer.release()\n",
        "\n",
        "\n",
        "  def run(self, output_path = \"output_video\"):\n",
        "\n",
        "    # Method to execute the complete video segmentation process and save the results.\n",
        "\n",
        "    # Open the video file using OpenCV.\n",
        "    cap = cv2.VideoCapture(self.file_path)\n",
        "    if not cap.isOpened():\n",
        "      raise ValueError(\"Error opening video file\")\n",
        "\n",
        "    i = 0     # Initialize frame index.\n",
        "    interval_video_F = []   # List to store segmented frames for the forehead.\n",
        "    interval_video_C1 = []  # List to store segmented frames for the first cheek.\n",
        "    interval_video_C2 = []  # List to store segmented frames for the second cheek.\n",
        "\n",
        "    # Convert timestamps to frame indexes.\n",
        "    indexes = [index//self.split_time for index in self.time_stamps]\n",
        "\n",
        "    # Process each frame of the video.\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "      time_ms = cap.get(cv2.CAP_PROP_POS_MSEC)  # Get the current frame timestamp in milliseconds.\n",
        "\n",
        "      # Check if the video has ended.\n",
        "      if not ret:\n",
        "        raise ValueError(\"Error reading frame\")\n",
        "        break\n",
        "\n",
        "      if i not in indexes:\n",
        "        # Resize the frame if its dimensions don't match the expected ones.\n",
        "        if int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) != self.image_height:\n",
        "          frame = cv2.resize(frame, (self.image_width, self.image_height))\n",
        "        try:\n",
        "          # Detect and get facial landmark coordinates.\n",
        "          coordinates = self.segment_frame(frame, time_ms)\n",
        "        except IndexError as e:\n",
        "          print(f\"{e}: No face detected at frame {cap.get(cv2.CAP_PROP_POS_FRAMES)}\")\n",
        "          cv2_imshow(frame)   # Display the frame where the face was not detected.\n",
        "          break\n",
        "\n",
        "        # Segment and store the video frames for each region (forehead, cheek 1, cheek 2).\n",
        "        interval_video_F.append(self.cut_video(frame, coordinates[0]))\n",
        "        interval_video_C1.append(self.cut_video(frame, coordinates[1]))\n",
        "        interval_video_C2.append(self.cut_video(frame, coordinates[2]))\n",
        "\n",
        "      # Save the segmented videos after processing each interval.\n",
        "      if len(interval_video_F) == self.split_time*self.fps:\n",
        "        self.save_video(np.array(interval_video_F), output_path + '_Forehead_' + str(i) + '.avi')\n",
        "        self.save_video(np.array(interval_video_C1), output_path + '_Cheek1_' + str(i) + '.avi')\n",
        "        self.save_video(np.array(interval_video_C2), output_path + '_Cheek2_' + str(i) + '.avi')\n",
        "        interval_video_F = []   # Reset the lists after saving.\n",
        "        interval_video_C1 = []\n",
        "        interval_video_C2 = []\n",
        "\n",
        "      # Update the frame index.\n",
        "      i = int(cap.get(cv2.CAP_PROP_POS_FRAMES)//(self.fps*self.split_time))\n",
        "\n",
        "      # Print the progress for every 1000th frame.\n",
        "      if cap.get(cv2.CAP_PROP_POS_FRAMES) % 1000 == 0:\n",
        "        print(f\"Frame {cap.get(cv2.CAP_PROP_POS_FRAMES)}\")\n",
        "\n",
        "      # Break the loop if the video exceeds 10 minutes.\n",
        "      if cap.get(cv2.CAP_PROP_POS_FRAMES) == (self.fps*60*10):\n",
        "        break\n",
        "\n",
        "    print(output_path + '. . . . . . . Done\\n')\n",
        "\n",
        "    # Release the video capture and close any OpenCV windows.\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDdujzJlxNsb"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jcVfPn_6mNAN"
      },
      "outputs": [],
      "source": [
        "def CreateDataset(array_like):\n",
        "\n",
        "  # Function that applies both the Video and Signal processing on the raw dataset.\n",
        "  # Saves the interval videos and a txt file with the HR groundtruth.\n",
        "\n",
        "  # Iterate through the given list.\n",
        "  for i, time in array_like:\n",
        "    in_path = '/content/drive/My Drive/Data Tese/Subject_' + str(i) + '/Subject_' + str(i)  # Define file paths.\n",
        "    out_path = '/content/drive/My Drive/Dataset/Subject_' + str(i) + '/Subject_' + str(i)\n",
        "\n",
        "    # Apply image and signal processing functions defined previously\n",
        "    SignalProcessing(in_path + '.txt', time_stamps = time).run(output_file = out_path)\n",
        "    #VideoSegmentation(in_path + '.mp4', time_stamps = time).run(output_path = out_path + '_')\n",
        "\n",
        "# Create a list where each component is a subsequent list with the subject number and time stamps to be removed.\n",
        "# This list represents the approach used for the original dataset used with this algorithm\n",
        "# and the timestamps here represent areas of the video/ECG with severe artifacts that the face detection or HR estimation could not handle.\n",
        "# Since a timestamp is composed of 20 seconds, any frame within this 20 seconds makes it skip the interval as a whole.\n",
        "# The number of the frames used are arbitrary numbers within the intervals that had to be removed\n",
        "# A list of all \"possible\" time stamps can be seen here: [10,30,50,70,90,110,130,150,170,190,210,230,250,270,290,310,330,350,370,390,410,430,450,470,490,510,530,550,570,590],\n",
        "# to a total of 30 intervals (30 * 20s = 600s = 10min).\n",
        "List = [[1,[]],[2,[470]],[3,[30,110,130,230]],[4,[30,50,190,210,250,290,570]],[5,[430]],[6,[]],[7,[]],[8,[]],[9,[]],[10,[10]],[11,[10]], [12,[]], [13,[10]]\n",
        "         , [14,[]], [15,[370,450,550,590]], [16,[]], [17,[]], [18,[370,430]], [19,[]], [20,[]], [21,[310,570,590]], [22,[]], [23,[]], [24,[]], [25,[]], [26,[]], [27,[50]]]\n",
        "\n",
        "CreateDataset(List)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iGvw7CAxa4U"
      },
      "source": [
        "# Heart Rate Predictiom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVHAJjJnyjtt"
      },
      "source": [
        "### Magnification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tgf91YCZWwdo"
      },
      "outputs": [],
      "source": [
        "class EVM:\n",
        "  def __init__(self, video_path, freq = [0.7, 2.5], amplification = 75, pyramid_level = 7, fps = 30, temporal_averaging_window = 10, magnify = False):\n",
        "\n",
        "    # Constructor to initialize the EVM class with parameters:\n",
        "    # video_path: Path to the input video file.\n",
        "    # freq: Tuple containing the lower and upper frequency bounds (in Hz) for bandpass filtering.\n",
        "    # amplification: Amplification factor to apply to the filtered signal.\n",
        "    # pyramid_level: Number of levels in the Gaussian pyramid.\n",
        "    # fps: Frames per second of the video.\n",
        "    # temporal_averaging window: size of the window for temporal averaging of the magnified video frames (0 for no averaging)\n",
        "    # magnify: True to output a manified video. False otherwise.\n",
        "\n",
        "    self.video_path = video_path\n",
        "    self.freq_low = freq[0]\n",
        "    self.freq_high = freq[1]\n",
        "    self.amp = amplification\n",
        "    self.levels = pyramid_level\n",
        "    self.fps = fps\n",
        "    self.mag = magnify\n",
        "    self.tav_win = temporal_averaging_window\n",
        "\n",
        "\n",
        "  def fft_filter(self, video):\n",
        "\n",
        "    # Apply FFT to the video, filter it based on frequency bounds, and amplify the result.\n",
        "    # Returns the filtered video.\n",
        "\n",
        "    # Perform FFT along the time axis (axis=0).\n",
        "    fft = fftpack.rfft(video, axis=0)\n",
        "\n",
        "    # Get the frequency spectrum corresponding to the FFT.\n",
        "    frequencies = fftpack.rfftfreq(fft.shape[0], d=1.0 / self.fps)\n",
        "\n",
        "    # Create a mask to keep only the frequencies within the specified range.\n",
        "    mask = np.logical_and(frequencies > self.freq_low, frequencies < self.freq_high)\n",
        "\n",
        "    # Zero out the frequencies outside the specified range.\n",
        "    fft[~mask] = 0\n",
        "\n",
        "    # Perform inverse FFT to get the filtered video.\n",
        "    filtered = fftpack.irfft(fft, axis=0)\n",
        "\n",
        "    # Amplify the filtered signal.\n",
        "    filtered *= self.amp\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "  def load_video(self, video_path):\n",
        "\n",
        "    # Load the video from the specified path and store its frames in a NumPy array.\n",
        "    # Returns the video frames.\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if the video file opened successfully.\n",
        "    if not cap.isOpened():\n",
        "      raise ValueError(\"Error opening video file\")\n",
        "\n",
        "    # Get the number of frames, width, and height of the video.\n",
        "    len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    image_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    image_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Create an empty array to store the video frames.\n",
        "    video_frames = np.empty((len, image_height, image_width, 3))\n",
        "\n",
        "    # Read each frame and convert it from BGR (OpenCV format) to RGB.\n",
        "    for x in range(len):\n",
        "      ret, frame = cap.read()\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      video_frames[x] = frame\n",
        "\n",
        "      # Check if there was an error reading the frame.\n",
        "      if not ret:\n",
        "        raise ValueError(\"Error reading frame\")\n",
        "        break\n",
        "\n",
        "    # Release the video capture object.\n",
        "    cap.release()\n",
        "\n",
        "    return video_frames\n",
        "\n",
        "\n",
        "  def save_video(self, video, output_path):\n",
        "\n",
        "    # Save video in the local files with name 'original name' + '_output.avi'\n",
        "\n",
        "    # Define the codec for the video writer using the fourcc code.\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "\n",
        "    # Get the height and width of the video frames.\n",
        "    [image_height, image_width] = video[0].shape[0:2]\n",
        "\n",
        "    # Initialize the VideoWriter object to write the video to the output path.\n",
        "    writer = cv2.VideoWriter(output_path, fourcc, self.fps, (image_width, image_height), 1)\n",
        "\n",
        "    # Iterate through each frame in the video array.\n",
        "    for i in range(video.shape[0]):\n",
        "      # Convert the frame from RGB format to BGR\n",
        "      frame = cv2.cvtColor(cv2.convertScaleAbs(video[i]), cv2.COLOR_RGB2BGR)\n",
        "      # Convert the frame to an 8-bit representation and write it to the video file.\n",
        "      writer.write(frame)\n",
        "\n",
        "    # Release the VideoWriter object\n",
        "    writer.release()\n",
        "\n",
        "\n",
        "  def rgb2yiq(self, video):\n",
        "\n",
        "    # Convert the video from RGB color space to YIQ color space.\n",
        "\n",
        "    yiq_from_rgb = np.array([[0.299, 0.587, 0.114],\n",
        "                            [0.596, -0.274, -0.322],\n",
        "                            [0.211, -0.523, 0.312]])\n",
        "\n",
        "    # Apply the RGB to YIQ conversion matrix.\n",
        "    t = np.dot(video, yiq_from_rgb.T)\n",
        "\n",
        "    return t\n",
        "\n",
        "\n",
        "  def yiq2rgb(self, video):\n",
        "\n",
        "    # Convert the video from YIQ color space back to RGB color space.\n",
        "\n",
        "    rgb_from_yiq = np.array([[1, 0.956, 0.621],\n",
        "                            [1, -0.272, -0.647],\n",
        "                            [1, -1.106, 1.703]])\n",
        "\n",
        "    # Apply the YIQ to RGB conversion matrix.\n",
        "    t = np.dot(video, rgb_from_yiq.T)\n",
        "\n",
        "    return t\n",
        "\n",
        "\n",
        "  def create_gaussian_pyramid(self, image, levels):\n",
        "\n",
        "    # Create a Gaussian pyramid with the specified number of levels for the given image.\n",
        "\n",
        "    gauss = image.copy()\n",
        "    gauss_pyr = [gauss]\n",
        "\n",
        "    # Iteratively downsample the image to create the pyramid.\n",
        "    for level in range(1, levels):\n",
        "      gauss = cv2.pyrDown(gauss)\n",
        "      gauss_pyr.append(gauss)\n",
        "\n",
        "    return gauss_pyr\n",
        "\n",
        "\n",
        "  def gaussian_video(self, video):\n",
        "\n",
        "    # Apply a Gaussian pyramid to each frame of the video and extract the smallest level.\n",
        "    # Returns the Gaussian video.\n",
        "\n",
        "    for i in range(0, video.shape[0]):\n",
        "      frame = video[i]\n",
        "      pyr = self.create_gaussian_pyramid(frame, self.levels)\n",
        "      gaussian_frame = pyr[-1]    # Use the smallest level of the pyramid.\n",
        "\n",
        "      # Initialize the array to store the Gaussian video if it's the first frame.\n",
        "      if i == 0:\n",
        "        gaussian_video = np.zeros((video.shape[0], gaussian_frame.shape[0], gaussian_frame.shape[1], 3))\n",
        "\n",
        "      gaussian_video[i] = gaussian_frame\n",
        "\n",
        "    return gaussian_video\n",
        "\n",
        "\n",
        "  def temporal_averaging(self, array, window_size):\n",
        "\n",
        "    # Apply temporal averaging to the video.\n",
        "    # Returns the video with a moving average over the frames.\n",
        "\n",
        "    # Create an empty array to store the smoothed frames\n",
        "    smoothed_array = np.zeros_like(array)\n",
        "\n",
        "    # Apply the moving average across the time dimension for each pixel\n",
        "    for t in range(array.shape[0]):\n",
        "\n",
        "      # Define the window: Ensure the window stays within bounds of the signal\n",
        "      start = max(0, t - window_size // 2)\n",
        "      end = min(array.shape[0], t + window_size // 2 + 1)\n",
        "\n",
        "      # Average over the window across time for each pixel\n",
        "      smoothed_array[t] = np.mean(array[start:end], axis=0)\n",
        "\n",
        "    return smoothed_array\n",
        "\n",
        "\n",
        "  def run(self):\n",
        "\n",
        "    # Main method to execute the Eulerian Video Magnification process.\n",
        "    # Returns the filtered video.\n",
        "\n",
        "    # Load the video.\n",
        "    video = self.load_video(self.video_path)\n",
        "\n",
        "    # Convert the video to YIQ color space.\n",
        "    yiq_video = self.rgb2yiq(video)\n",
        "\n",
        "    # Apply a Gaussian pyramid to the video.\n",
        "    gaussian_video = self.gaussian_video(yiq_video)\n",
        "\n",
        "    # Apply the FFT-based filter to the Gaussian video.\n",
        "    filtered = self.fft_filter(gaussian_video)\n",
        "\n",
        "    # Convert the filtered video back to RGB color space.\n",
        "    filtered = self.yiq2rgb(filtered)\n",
        "\n",
        "    # Apply temporal averaging to each one of the channels (R, G, B)\n",
        "    if self.tav_win != 0:\n",
        "      filtered_mag = self.temporal_averaging(filtered, self.tav_win)\n",
        "    else:\n",
        "      filtered_mag = filtered\n",
        "\n",
        "    # If magnification is enabled, apply magnification to the filtered video.\n",
        "    if self.mag:\n",
        "      filtered_video = np.zeros(video.shape) # Create array with original video's shape\n",
        "      for i in range(video.shape[0]):\n",
        "          f = filtered_mag[i]\n",
        "          filtered_video[i] = cv2.resize(f, (video.shape[2], video.shape[1]))\n",
        "      filtered_video += video    # Add original video to the filtered video\n",
        "      filtered_video[filtered_video < 0] = 0\n",
        "      filtered_video = (filtered_video/np.max(filtered_video)) * 255\n",
        "      self.save_video(filtered_video, 'filtered.avi')    # Save the filtered video\n",
        "\n",
        "    # Normalize the pixel values of each channel to ensure they are in [0,1] range for the DL model.\n",
        "    # This part of the code is prepared to have an input of an image 1 by 1.\n",
        "    # If this condition is not met then an average of the image pixels is calculated.\n",
        "    # This is mainly important due to the nature of the Deep Learning Approach. If the only aim is to\n",
        "    # magnify videos then this block can be commented out.\n",
        "\n",
        "    if np.shape(filtered)[1] != 1:\n",
        "      filtered = np.mean(filtered, axis=(1, 2))\n",
        "      filtered = np.expand_dims(filtered, axis=(1, 2))\n",
        "\n",
        "    filtered = np.transpose(filtered, (3, 0, 1, 2)) # Rearange the data axis\n",
        "    filtered = filtered.reshape(filtered.shape[0], filtered.shape[1]) # Reshape to flatten the 1 by 1 dimensions\n",
        "    filtered = np.array([(channel - np.min(channel))/(np.max(channel) - np.min(channel)) for channel in filtered]) # Normalize each channel\n",
        "\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6tvFqUvMTgC"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ElLU5MjS6fI1"
      },
      "outputs": [],
      "source": [
        "class DataPreparation(Dataset):\n",
        "  def __init__(self, subjects, videos, heart_rates):\n",
        "\n",
        "    # Constructor to initialize the dataset with the provided data.\n",
        "    # subjects: List or array of subject identifiers.\n",
        "    # videos: List or array of video data, where each element corresponds to a subject.\n",
        "    # heart_rates: List or array of heart rate data, where each element corresponds to a subject.\n",
        "\n",
        "    self.subjects = subjects\n",
        "    self.videos = videos\n",
        "    self.heart_rates = heart_rates\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    # Returns the total number of samples in the dataset.\n",
        "\n",
        "    return len(self.subjects)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    # Returns the video and heart rate data at the specified index.\n",
        "\n",
        "    video = self.videos[idx]    # Get the video data at the given index.\n",
        "    heart_rate = self.heart_rates[idx]  # Get the heart rate data at the given index.\n",
        "\n",
        "    # Convert the heart rate data to a torch tensor of type float32.\n",
        "    return torch.tensor(video, dtype=torch.float32), torch.tensor(heart_rate, dtype=torch.float32)\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "  def __init__(self, patience=5, min_delta=0):\n",
        "\n",
        "    # Constructor to initialize the early stopping mechanism.\n",
        "    # patience: Number of epochs to wait after the last improvement before stopping.\n",
        "    # min_delta: Minimum change in the monitored quantity to qualify as an improvement.\n",
        "\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0    # Counter to track the number of epochs without improvement.\n",
        "    self.best_loss = None    # Best loss value observed so far.\n",
        "    self.early_stop = False    # Flag to indicate if early stopping should be triggered.\n",
        "\n",
        "  def __call__(self, val_loss):\n",
        "\n",
        "    # Method to be called at each epoch to check whether early stopping should be triggered.\n",
        "\n",
        "    # Initialize best_loss if it hasn't been set yet\n",
        "    if self.best_loss is None:\n",
        "      self.best_loss = val_loss\n",
        "    # Check if validation loss has increased more than min_delta\n",
        "    elif val_loss > self.best_loss - self.min_delta:\n",
        "      self.counter += 1   # Increment the counter if no improvement is observed.\n",
        "      # If counter exceeds patience, set early_stop to True\n",
        "      if self.counter >= self.patience:\n",
        "        self.early_stop = True\n",
        "    else:\n",
        "      # If there is an improvement, update the best_loss and reset the counter.\n",
        "      self.best_loss = val_loss\n",
        "      self.counter = 0\n",
        "\n",
        "\n",
        "def moving_average(data, window_size):\n",
        "#Compute the moving average of the input data with the given window size.\n",
        "\n",
        "  return np.convolve(data, np.ones(window_size)/window_size, mode='valid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7DYlDcxU-t"
      },
      "source": [
        "### Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq8OyuaENdWI"
      },
      "outputs": [],
      "source": [
        "num_subjects = 27   # Total number of subjects in the dataset.\n",
        "videos = []        # List to store video data (commented out).\n",
        "subject_num = []    # List to store subject numbers.\n",
        "heart_rate = []     # List to store corresponding heart rate values.\n",
        "mean_hr = []       # List to store the mean heart rate for each subject.\n",
        "\n",
        "# Loop over each subject's interval videos and txt files to process their data.\n",
        "for i in range(1, num_subjects+1):\n",
        "  # Load the heart rate information for the current subject.\n",
        "  info = np.loadtxt('/content/drive/My Drive/Dataset/Subject_'+ str(i) +'/Subject_' + str(i) + '_HR.txt')\n",
        "  counter = 0\n",
        "  hr = 0\n",
        "\n",
        "  # Loop over each entry in the heart rate info.\n",
        "  for j in range(len(info)):\n",
        "    if info[j][1] != 0:   # Only consider intervals where the heart rate is not zero\n",
        "      video = EVM('/content/drive/My Drive/Dataset/Subject_'+ str(i) +'/Subject_' + str(i) + '__Forehead_' + str(j) + '.avi').run()\n",
        "      video = np.array([moving_average(channel, 10) for channel in video])\n",
        "      videos.append(torch.tensor(video, dtype=torch.float32))\n",
        "\n",
        "      # Save mean HR per subject to ensure an even distribution later:\n",
        "      hr += info[j][1]\n",
        "      counter += 1\n",
        "\n",
        "      # Append subject number, heart rate to their respective lists.\n",
        "      subject_num.append(i)\n",
        "      heart_rate.append(info[j][1])\n",
        "\n",
        "  mean_hr.append(hr/counter)\n",
        "  print(\"Subject \" + str(i) + \" is done\")   # Print a message when processing for a subject is completed.\n",
        "\n",
        "# Convert to Array\n",
        "subject_num = np.array(subject_num)\n",
        "heart_rate = np.array(heart_rate)\n",
        "mean_hr = np.array(mean_hr)\n",
        "\n",
        "# Save the processed video data (commented out).\n",
        "np.save(\"videos.npy\", videos)\n",
        "\n",
        "# Load the saved video data.\n",
        "videos = np.load(\"videos.npy\")\n",
        "videos = np.array(videos)   # Convert the list of videos to a NumPy array.\n",
        "\n",
        "# Take unique subjects to perform subject based division:\n",
        "subjects = np.unique(subject_num)\n",
        "\n",
        "# Organize in bins to use stratify\n",
        "_, bins = np.histogram(mean_hr, 4)\n",
        "bins = np.digitize(mean_hr, bins[1:4])\n",
        "\n",
        "# Perform subject division in order to create train, validation and test sets\n",
        "subjects_train, subjects_temp, bins_train, bins_temp = train_test_split(subjects, bins, test_size=0.4,\n",
        "                                                                              random_state=42, stratify=bins)\n",
        "subjects_val, subjects_test = train_test_split(subjects_temp, test_size=0.5, random_state=42, stratify=bins_temp)\n",
        "\n",
        "# Calculate dataset indices\n",
        "idx_train = np.isin(subject_num, subjects_train)\n",
        "idx_val = np.isin(subject_num, subjects_val)\n",
        "idx_test = np.isin(subject_num, subjects_test)\n",
        "\n",
        "# Create dataset instances for training, validation, and testing.\n",
        "train_dataset = DataPreparation(subject_num[idx_train], videos[idx_train], heart_rate[idx_train])\n",
        "val_dataset = DataPreparation(subject_num[idx_val], videos[idx_val], heart_rate[idx_val])\n",
        "test_dataset = DataPreparation(subject_num[idx_test], videos[idx_test], heart_rate[idx_test])\n",
        "\n",
        "# Create data loaders for batching and shuffling the datasets.\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP19-mplbOTi"
      },
      "outputs": [],
      "source": [
        "# Example of BVP signal with the Green Channel (index 0 for Red, 1 for Green and 2 for Blue)\n",
        "\n",
        "example = videos[0]\n",
        "\n",
        "#plt.plot(example[0])\n",
        "plt.plot(example[1])\n",
        "#plt.plot(example[2])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# In this case 20 obvious peaks can be identified, leading to a rough estimate of 60 bpm (Real: 62 bpm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model"
      ],
      "metadata": {
        "id": "jFTbly5ND-IG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scElbp6P6sIe"
      },
      "outputs": [],
      "source": [
        "# Develop a baseline method to evaluate the Deep Learning Network performance.\n",
        "\n",
        "# Extract the green channel (second channel) from the video data for all samples.\n",
        "data = videos[:,1,:]\n",
        "\n",
        "# Reshape the data to collapse the spatial dimensions (width and height) into a single dimension.\n",
        "data = data.reshape(data.shape[0], -1)\n",
        "\n",
        "result = []   # List to store the estimated heart rates for each video.\n",
        "\n",
        "# Loop over each video in the dataset.\n",
        "for vid in data:\n",
        "  # Detect peaks in the green channel signal; 'distance=14' ensures that peaks are not too close together.\n",
        "  peaks, _ = signal.find_peaks(vid, distance=14)\n",
        "\n",
        "  # Calculate the time intervals (in seconds) between consecutive peaks (RR intervals).\n",
        "  rr_intervals = np.diff(peaks)/30    # Assuming a frame rate of 30 FPS.\n",
        "\n",
        "  # Calculate heart rates (beats per minute) based on RR intervals.\n",
        "  hearts = 60 / rr_intervals\n",
        "\n",
        "  # Compute the mean heart rate for the video and round it to the nearest integer.\n",
        "  mean_hr = round(np.mean(hearts))\n",
        "\n",
        "  # Append the estimated mean heart rate to the results list.\n",
        "  result.append(mean_hr)\n",
        "\n",
        "# Initialize error metrics: Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and Root Mean Squared Error (RMSE).\n",
        "MAE = 0\n",
        "MAPE = 0\n",
        "RMSE = 0\n",
        "\n",
        "# Calculate the error metrics.\n",
        "for i in range(len(heart_rate)):\n",
        "\n",
        "  # Compute the absolute difference between the estimated and true heart rates.\n",
        "  MAE += abs(result[i] - heart_rate[i])\n",
        "\n",
        "  # Compute the absolute percentage error.\n",
        "  MAPE += abs(result[i] - heart_rate[i])/heart_rate[i]\n",
        "\n",
        "  # Compute the squared error.\n",
        "  RMSE += (result[i] - heart_rate[i])**2\n",
        "\n",
        "# Average the MAE over the videos.\n",
        "MAE = MAE/len(heart_rate)\n",
        "\n",
        "# Average the MAPE over the videos and convert it to a percentage.\n",
        "MAPE = (MAPE/len(heart_rate))*100\n",
        "\n",
        "# Calculate the RMSE by taking the square root of the average squared error.\n",
        "RMSE = np.sqrt(RMSE/len(heart_rate))\n",
        "\n",
        "# Print the calculated error metrics.\n",
        "print(\"MAE:\", MAE)\n",
        "print(\"MAPE:\", MAPE)\n",
        "print(\"RMSE:\", RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M61J6EGExfjr"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gAwsdA4D1PKV"
      },
      "outputs": [],
      "source": [
        "class DenormalizeLayer(nn.Module):\n",
        "  def __init__(self, original_min=40, original_max=150):\n",
        "    super(DenormalizeLayer, self).__init__()\n",
        "\n",
        "    # Define the range to denormalize values back to their original scale\n",
        "    self.original_min = original_min\n",
        "    self.original_max = original_max\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Convert normalized values back to the original scale\n",
        "    return x * (self.original_max - self.original_min) + self.original_min\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TemporalCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TemporalCNN, self).__init__()\n",
        "\n",
        "    # Define the first convolutional block\n",
        "    self.conv11 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=12, padding=1)\n",
        "    self.conv12 = nn.Conv1d(in_channels=8, out_channels=8, kernel_size=1, padding=1)\n",
        "\n",
        "    # Define the second convolutional block\n",
        "    self.conv21 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=24, padding=1)\n",
        "    self.conv22 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=1, padding=1)\n",
        "\n",
        "    # Batch normalization layers for each convolutional block\n",
        "    self.bn1 = nn.BatchNorm1d(8,track_running_stats=False)\n",
        "    self.bn2 = nn.BatchNorm1d(16,track_running_stats=False)\n",
        "\n",
        "    # Activation function and dropout layer\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Apply the first convolutional block followed by batch normalization, ReLU, and dropout\n",
        "    x = self.bn1(self.conv12(self.conv11(x)))\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # Apply the second convolutional block followed by batch normalization, ReLU, and dropout\n",
        "    x = self.bn2(self.conv22(self.conv21(x)))\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CombinedModel, self).__init__()\n",
        "\n",
        "    # Initialize the temporal CNN and LSTM layers\n",
        "    self.temporal_cnn = TemporalCNN()\n",
        "    self.lstm = nn.LSTM(input_size=16, hidden_size=16, batch_first=True)\n",
        "\n",
        "    # Fully connected (dense) layer\n",
        "    self.fc = nn.Linear(16, 1)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.denormalize = DenormalizeLayer()\n",
        "\n",
        "    # Initialize weights\n",
        "    self.initialize_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Reshape the input to match the expected shape of (batch_size, channels, time)\n",
        "    batch_size, c, t = x.shape\n",
        "\n",
        "    # Pass the data through the TemporalCNN\n",
        "    x = self.temporal_cnn(x)\n",
        "\n",
        "    # Reshape the output to match the input size of the LSTM\n",
        "    x = x.view(batch_size, -1, 16)\n",
        "\n",
        "    # Pass the data through the LSTM\n",
        "    lstm_out, _ = self.lstm(x)\n",
        "\n",
        "    # Use the output from the last time step of the LSTM\n",
        "    x = lstm_out[:, -1, :]\n",
        "\n",
        "    # Pass through a fully connected layer\n",
        "    x = self.fc(x)\n",
        "\n",
        "    # Apply sigmoid activation function and denormalize the output\n",
        "    x = self.sigmoid(x)\n",
        "    x = self.denormalize(x)\n",
        "    return x.squeeze()\n",
        "\n",
        "\n",
        "  def initialize_weights(self):\n",
        "\n",
        "    # Initialize weights for Conv1d, Linear, and LSTM layers\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.LSTM):\n",
        "        for name, param in m.named_parameters():\n",
        "          if 'weight_ih' in name:\n",
        "            nn.init.xavier_uniform_(param.data)\n",
        "          elif 'weight_hh' in name:\n",
        "            nn.init.orthogonal_(param.data)\n",
        "          elif 'bias' in name:\n",
        "            param.data.fill_(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4S_hpZSxklI"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wErgz7B7sAUX"
      },
      "outputs": [],
      "source": [
        "# Initialize the Model.\n",
        "model = CombinedModel()\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU).\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define loss function, optimizer, and early stopping criteria.\n",
        "criterion = nn.SmoothL1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.02)    # Adam optimizer for training.\n",
        "early_stopping = EarlyStopping(patience=30, min_delta=0.01)    # Early stopping to prevent overfitting.\n",
        "\n",
        "# Set the number of training epochs.\n",
        "num_epochs = 1000\n",
        "\n",
        "# Move model to the specified device (GPU or CPU).\n",
        "model = model.to(device)\n",
        "\n",
        "# Lists to store training and validation loss for plotting.\n",
        "loss_train_plot = []\n",
        "loss_val_plot = []\n",
        "previous = 200\n",
        "\n",
        "#Model Training.\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()   # Set the model to training mode.\n",
        "  running_loss = 0.0    # Initialize running loss for this epoch.\n",
        "\n",
        "  # Iterate over training data.\n",
        "  for videos, scalars in train_loader:\n",
        "    videos, scalars = videos.to(device), scalars.to(device)   # Move data to device.\n",
        "    optimizer.zero_grad()   # Zero the parameter gradients.\n",
        "    outputs = model(videos)   # Forward pass.\n",
        "    loss = criterion(outputs, scalars)    # Compute loss.\n",
        "    loss.backward()   # Backward pass.\n",
        "    optimizer.step()    # Update weights.\n",
        "    running_loss += loss.item()   # Accumulate loss.\n",
        "\n",
        "  # Compute average training loss for this epoch.\n",
        "  train_loss = running_loss / len(train_loader)\n",
        "  loss_train_plot.append(train_loss)    # Store for plotting .\n",
        "\n",
        "  # Evaluate the model on the validation set.\n",
        "  model.eval()    # Set the model to evaluation mode.\n",
        "  val_loss = 0.0\n",
        "  MAE = []    # List to store Mean Absolute Error values.\n",
        "  MAPE = []   # List to store Mean Absolute Percentage Error values.\n",
        "  RMSE = []   # List to store Root Mean Squared Error values.\n",
        "\n",
        "  with torch.no_grad():   # No need to compute gradients for validation.\n",
        "    for videos, scalars in val_loader:\n",
        "      videos, scalars = videos.to(device), scalars.to(device)   # Move data to device.\n",
        "      outputs = model(videos)   # Forward pass\n",
        "      loss_val = criterion(outputs, scalars)    # Compute validation loss.\n",
        "      val_loss += loss_val.item()   # Accumulate validation loss.\n",
        "\n",
        "      # Calculate metrics for each sample in the val set.\n",
        "      for pred, value in zip(outputs, scalars):\n",
        "        pred_flat = pred.view(-1).cpu().numpy()   # Flatten and move to CPU.\n",
        "        value_flat = value.view(-1).cpu().numpy()  # Flatten and move to CPU.\n",
        "\n",
        "        MAE.append(abs(pred_flat - value_flat))    # Mean Absolute Error.\n",
        "        MAPE.append(abs(pred_flat - value_flat)/value_flat)   # Mean Absolute Percentage Error.\n",
        "        RMSE.append((pred_flat - value_flat)**2)   # Root Mean Squared Error.\n",
        "\n",
        "  # Compute average validation loss and metrics for the epoch.\n",
        "  val_loss_final = val_loss / len(val_loader)\n",
        "  loss_val_plot.append(val_loss_final)    # Store for plotting.\n",
        "  val_MAE = np.mean(MAE)\n",
        "  val_MAPE = np.mean(MAPE)*100\n",
        "  val_RMSE = np.sqrt(np.mean(RMSE))\n",
        "\n",
        "  # Print training and validation loss for the epoch.\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss_final:.3f}')\n",
        "  torch.cuda.empty_cache()    # Clear GPU memory\n",
        "\n",
        "  # Update best model if validation loss improves.\n",
        "  if val_loss_final < previous:\n",
        "    previous = val_loss_final\n",
        "    torch.save(model.state_dict(), 'best-model.pt')\n",
        "    best_mae = val_MAE\n",
        "    best_mape = val_MAPE\n",
        "    best_rmse = val_RMSE\n",
        "\n",
        "  # Check for early stopping.\n",
        "  early_stopping(val_loss_final)\n",
        "  if early_stopping.early_stop:\n",
        "    print(\"Early stopping triggered\")\n",
        "    break\n",
        "\n",
        "print(f'\\nBest Model Loss: {previous:.3f}, MAE: {best_mae:.3f}, MAPE: {best_mape:.3f},  RMSE: {best_rmse:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9KWx8bXQNBL"
      },
      "outputs": [],
      "source": [
        "# Define the window size for the moving average\n",
        "window_size = 20\n",
        "\n",
        "# Calculate moving averages\n",
        "train_loss_smooth = moving_average(loss_train_plot, window_size)\n",
        "val_loss_smooth = moving_average(loss_val_plot, window_size)\n",
        "\n",
        "# Adjust the x-axis range to match the length of the smoothed data\n",
        "x_axis_train = range(window_size, len(loss_train_plot) + 1)\n",
        "x_axis_val = range(window_size, len(loss_val_plot) + 1)\n",
        "\n",
        "# Create a new figure for plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot smoothed training loss\n",
        "plt.plot(x_axis_train, train_loss_smooth, label='Smoothed Training Loss', color='blue')\n",
        "\n",
        "# Plot smoothed validation loss\n",
        "plt.plot(x_axis_val, val_loss_smooth, label='Smoothed Validation Loss', color='orange')\n",
        "\n",
        "# Add title and labels\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Testing"
      ],
      "metadata": {
        "id": "ilO6zaSDEDiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model weights.\n",
        "model.load_state_dict(torch.load('best-model.pt', weights_only = True))\n",
        "\n",
        "# Evaluation on the test set.\n",
        "model.eval()    # Set the model to evaluation mode.\n",
        "test_loss = 0.0\n",
        "MAE = []    # List to store Mean Absolute Error values.\n",
        "MAPE = []   # List to store Mean Absolute Percentage Error values.\n",
        "RMSE = []   # List to store Root Mean Squared Error values.\n",
        "\n",
        "with torch.no_grad():   # No need to compute gradients for testing.\n",
        "  for videos, scalars in test_loader:\n",
        "    videos, scalars = videos.to(device), scalars.to(device)   # Move data to device.\n",
        "    outputs = model(videos)   # Forward pass.\n",
        "    loss = criterion(outputs, scalars)    # Compute test loss.\n",
        "    test_loss += loss.item()    # Accumulate test loss.\n",
        "\n",
        "    # Calculate metrics for each sample in the test set.\n",
        "    for pred, value in zip(outputs, scalars):\n",
        "      pred_flat = pred.view(-1).cpu().numpy()   # Flatten and move to CPU.\n",
        "      value_flat = value.view(-1).cpu().numpy()  # Flatten and move to CPU.\n",
        "\n",
        "      MAE.append(np.round(abs(pred_flat - value_flat)))    # Mean Absolute Error.\n",
        "      MAPE.append(abs(pred_flat - value_flat)/value_flat)   # Mean Absolute Percentage Error.\n",
        "      RMSE.append((pred_flat - value_flat)**2)   # Root Mean Squared Error.\n",
        "\n",
        "# Compute average test loss and metrics.\n",
        "test_loss = test_loss / len(test_loader)\n",
        "test_MAE = np.mean(MAE)\n",
        "test_MAPE = np.mean(MAPE)*100\n",
        "test_RMSE = np.sqrt(np.mean(RMSE))\n",
        "\n",
        "# Print test metrics.\n",
        "print(f'Test Loss: {test_loss:.3f}, Test MAE: {test_MAE:.3f}, Test MAPE: {test_MAPE:.3f},  Test RMSE: {test_RMSE:.3f}')"
      ],
      "metadata": {
        "id": "uR0YTu6SuvfN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_6S6AsfejDxc",
        "lU2gJNY-i2Dm",
        "jnM_mQjpxmQ2",
        "-IKJsgwtDz0m",
        "xDdujzJlxNsb",
        "0iGvw7CAxa4U",
        "EVHAJjJnyjtt",
        "a6tvFqUvMTgC",
        "Rj7DYlDcxU-t",
        "jFTbly5ND-IG",
        "M61J6EGExfjr",
        "B4S_hpZSxklI",
        "ilO6zaSDEDiJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}